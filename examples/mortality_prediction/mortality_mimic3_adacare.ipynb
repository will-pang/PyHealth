{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe8e8002",
   "metadata": {},
   "source": [
    "## Step 1: Load the MIMIC-III Dataset\n",
    "\n",
    "We'll load the MIMIC-III dataset using PyHealth 2's new `MIMIC3Dataset` class. We need to specify:\n",
    "- `root`: Path to the MIMIC-III data directory\n",
    "- `tables`: Clinical tables to load (diagnoses, procedures, prescriptions)\n",
    "- `dev`: Set to `True` for development/testing with a small subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e13ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.datasets import MIMIC3Dataset\n",
    "\n",
    "# Load MIMIC-III dataset\n",
    "dataset = MIMIC3Dataset(\n",
    "    root=r\"F:\\coding_projects\\pyhealth\\downloads\\mimic-iii-demo\",\n",
    "    tables=[\"diagnoses_icd\", \"procedures_icd\", \"prescriptions\"],\n",
    "    dev=True,  # Set to False for full dataset\n",
    ")\n",
    "\n",
    "dataset.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a68680",
   "metadata": {},
   "source": [
    "## Step 2: Define the Mortality Prediction Task\n",
    "\n",
    "PyHealth 2 uses task classes to define how to extract samples from the raw EHR data. The `MortalityPredictionMIMIC3` task:\n",
    "- Extracts diagnosis codes (ICD-9), procedure codes, and drug information from each visit\n",
    "- Creates binary labels based on mortality in the next visit\n",
    "- Filters out visits without sufficient clinical codes\n",
    "\n",
    "You can optionally specify a `cache_dir` to save processed samples for faster future loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48072b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.tasks import MortalityPredictionMIMIC3\n",
    "\n",
    "# Define the mortality prediction task\n",
    "task = MortalityPredictionMIMIC3()\n",
    "\n",
    "# Apply the task to generate samples\n",
    "samples = dataset.set_task(\n",
    "    task=task,\n",
    "    cache_dir=\"./cache_mortality_mimic3\"  # Cache processed samples\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(samples)} samples\")\n",
    "print(f\"\\nInput schema: {samples.input_schema}\")\n",
    "print(f\"Output schema: {samples.output_schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46810ca9",
   "metadata": {},
   "source": [
    "## Step 3: Explore a Sample\n",
    "\n",
    "Let's examine what a single sample looks like. Each sample represents one hospital visit with:\n",
    "- **conditions**: List of ICD-9 diagnosis codes\n",
    "- **procedures**: List of ICD-9 procedure codes  \n",
    "- **drugs**: List of drug names\n",
    "- **mortality**: Binary label (0 = survived, 1 = deceased in next visit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc5a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample\n",
    "print(\"Sample structure:\")\n",
    "print(samples[0])\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Dataset Statistics:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Count unique codes\n",
    "all_conditions = set()\n",
    "all_procedures = set()\n",
    "all_drugs = set()\n",
    "mortality_count = 0\n",
    "for sample in samples:\n",
    "    all_conditions.update(sample['conditions'])\n",
    "    all_procedures.update(sample['procedures'])\n",
    "    all_drugs.update(sample['drugs'])\n",
    "    mortality_count += float(sample['mortality'])\n",
    "\n",
    "print(f\"Unique diagnosis codes: {len(all_conditions)}\")\n",
    "print(f\"Unique procedure codes: {len(all_procedures)}\")\n",
    "print(f\"Unique drugs: {len(all_drugs)}\")\n",
    "print(f\"\\nMortality rate: {mortality_count/len(samples)*100:.2f}%\")\n",
    "print(f\"Positive samples: {mortality_count}\")\n",
    "print(f\"Negative samples: {len(samples) - mortality_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f669091",
   "metadata": {},
   "source": [
    "## Step 4: Split the Dataset\n",
    "\n",
    "We split the data into training, validation, and test sets using a 70-10-20 split.\n",
    "\n",
    "**Note:** We use `split_by_sample` which randomly splits samples. For time-series tasks, you might want to use temporal splits to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d00270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.datasets import split_by_sample\n",
    "\n",
    "# Split dataset: 70% train, 10% validation, 20% test\n",
    "train_dataset, val_dataset, test_dataset = split_by_sample(\n",
    "    dataset=samples, \n",
    "    ratios=[0.7, 0.1, 0.2]\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb55295f",
   "metadata": {},
   "source": [
    "## Step 5: Create Data Loaders\n",
    "\n",
    "Data loaders batch the samples and handle data feeding during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cb63ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.datasets import get_dataloader\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = get_dataloader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = get_dataloader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_dataloader = get_dataloader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_dataloader)}\")\n",
    "print(f\"Validation batches: {len(val_dataloader)}\")\n",
    "print(f\"Test batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280d362",
   "metadata": {},
   "source": [
    "## Step 6: Initialize the AdaCare Model\n",
    "\n",
    "The AdaCare model in PyHealth 2 automatically handles different feature types:\n",
    "- **Sequence features** (like diagnosis/procedure/drug codes) are embedded using learned embeddings\n",
    "- **Multiple feature keys** are processed by separate AdaCare layers\n",
    "- The model provides interpretability through attention weights\n",
    "\n",
    "### Key Parameters:\n",
    "- `embedding_dim`: Dimension of code embeddings (default: 128)\n",
    "- `hidden_dim`: Hidden dimension of GRU layers (default: 128)\n",
    "- `kernel_size`: Kernel size for causal convolution (default: 2)\n",
    "- `kernel_num`: Number of convolution kernels (default: 64)\n",
    "- `dropout`: Dropout rate for regularization (default: 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c60e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.models import AdaCare\n",
    "\n",
    "# Initialize AdaCare model\n",
    "model = AdaCare(\n",
    "    dataset=samples,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=128,\n",
    ")\n",
    "\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9f4817",
   "metadata": {},
   "source": [
    "## Step 7: Train the Model\n",
    "\n",
    "We use PyHealth's `Trainer` class which handles:\n",
    "- Training loop with automatic batching\n",
    "- Validation during training\n",
    "- Model checkpointing based on validation metrics\n",
    "- Early stopping\n",
    "\n",
    "We monitor the **ROC-AUC** score on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac11472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.trainer import Trainer\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    metrics=[\"roc_auc\", \"pr_auc\", \"accuracy\", \"f1\"]  # Track multiple metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train(\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    epochs=50,\n",
    "    monitor=\"roc_auc\",  # Use ROC-AUC for model selection\n",
    "    optimizer_params={\"lr\": 1e-3},  # Learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2699323a",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate on Test Set\n",
    "\n",
    "After training, we evaluate the model on the held-out test set to measure its generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea47a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(test_dataloader)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Test Set Performance\")\n",
    "print(\"=\"*50)\n",
    "for metric, value in test_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d0224b",
   "metadata": {},
   "source": [
    "## Step 9: Model Interpretability (Optional)\n",
    "\n",
    "One of AdaCare's key features is interpretability. The model provides attention weights that indicate which features are most important for predictions.\n",
    "\n",
    "Let's examine the feature importance for a few test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Get a batch from test set\n",
    "test_batch = next(iter(test_dataloader))\n",
    "\n",
    "# Run model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(**test_batch)\n",
    "\n",
    "# Extract interpretability information\n",
    "if 'feature_importance' in output:\n",
    "    print(\"Feature importance available!\")\n",
    "    print(f\"Shape: {output['feature_importance']}\")\n",
    "    \n",
    "    # Display importance for first sample\n",
    "    print(\"\\nFeature importance for first sample:\")\n",
    "    print(\"This shows which clinical features the model focuses on.\")\n",
    "else:\n",
    "    print(\"Feature importance not available in model output.\")\n",
    "\n",
    "# Display predictions\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\"*50)\n",
    "predictions = output['y_prob'].cpu().numpy()\n",
    "true_labels = output['y_true'].cpu().numpy()\n",
    "\n",
    "for i in range(min(5, len(predictions))):\n",
    "    pred = predictions[i][0]\n",
    "    true = int(true_labels[i][0])\n",
    "    print(f\"Sample {i+1}: Predicted={pred:.3f}, True={true}, Prediction={'Mortality' if pred > 0.5 else 'Survival'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
