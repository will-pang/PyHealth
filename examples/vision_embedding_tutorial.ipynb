{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad9b5059",
   "metadata": {},
   "source": [
    "# VisionEmbeddingModel Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the `VisionEmbeddingModel` for medical imaging tasks in PyHealth.\n",
    "\n",
    "**Contributors:** Josh Steier \n",
    "\n",
    "\n",
    "**Overview:**\n",
    "- Load a medical imaging dataset (MIMIC-CXR or custom)\n",
    "- Configure the `VisionEmbeddingModel` with different backbones\n",
    "- Build an end-to-end classification pipeline\n",
    "- Train and evaluate on chest X-ray classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c778a6a",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Configure deterministic behavior and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272cc5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "\n",
    "from pyhealth.datasets import create_sample_dataset\n",
    "from pyhealth.datasets.splitter import split_by_patient\n",
    "from pyhealth.datasets.utils import get_dataloader\n",
    "from pyhealth.processors import ImageProcessor\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8879729",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "**Option A:** Synthetic data (default, no download required)\n",
    "\n",
    "**Option B:** MIMIC-CXR (requires PhysioNet credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SYNTHETIC = True\n",
    "MIMIC_CXR_ROOT = \"/path/to/physionet.org/files/mimic-cxr-jpg/2.0.0\"\n",
    "\n",
    "if USE_SYNTHETIC:\n",
    "    DATA_ROOT = tempfile.mkdtemp(prefix=\"chest_xray_\")\n",
    "    images_dir = os.path.join(DATA_ROOT, \"images\")\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    \n",
    "    samples = []\n",
    "    for i in range(200):\n",
    "        label = i % 2\n",
    "        img_array = np.random.normal(80, 25, (224, 224))\n",
    "        y, x = np.ogrid[:224, :224]\n",
    "        left_mask = ((x - 72)**2 / 3000 + (y - 112)**2 / 8000) < 1\n",
    "        right_mask = ((x - 152)**2 / 3000 + (y - 112)**2 / 8000) < 1\n",
    "        img_array[left_mask] -= 20\n",
    "        img_array[right_mask] -= 20\n",
    "        \n",
    "        if label == 1:\n",
    "            cx, cy = 112 + np.random.randint(-50, 50), 112 + np.random.randint(-30, 30)\n",
    "            radius = np.random.randint(15, 40)\n",
    "            opacity_mask = (x - cx)**2 + (y - cy)**2 <= radius**2\n",
    "            img_array[opacity_mask] += 60 + np.random.randint(0, 40)\n",
    "        \n",
    "        img_array = np.clip(img_array, 0, 255).astype(np.uint8)\n",
    "        img = Image.fromarray(img_array, mode='L')\n",
    "        img_path = os.path.join(images_dir, f\"cxr_{i:04d}.png\")\n",
    "        img.save(img_path)\n",
    "        \n",
    "        samples.append({\n",
    "            \"patient_id\": f\"p{i // 4}\",\n",
    "            \"visit_id\": f\"v{i}\",\n",
    "            \"image\": img_path,\n",
    "            \"label\": label,\n",
    "        })\n",
    "    \n",
    "    print(f\"Created {len(samples)} synthetic images in {DATA_ROOT}\")\n",
    "    \n",
    "else:\n",
    "    DATA_ROOT = MIMIC_CXR_ROOT\n",
    "    import pandas as pd\n",
    "    \n",
    "    metadata = pd.read_csv(f\"{DATA_ROOT}/mimic-cxr-2.0.0-metadata.csv\")\n",
    "    labels = pd.read_csv(f\"{DATA_ROOT}/mimic-cxr-2.0.0-chexpert.csv\")\n",
    "    df = metadata.merge(labels, on=[\"subject_id\", \"study_id\"])\n",
    "    \n",
    "    def build_path(row):\n",
    "        subject = f\"p{row['subject_id']}\"\n",
    "        return f\"{DATA_ROOT}/files/{subject[:3]}/{subject}/s{row['study_id']}/{row['dicom_id']}.jpg\"\n",
    "    \n",
    "    df[\"image_path\"] = df.apply(build_path, axis=1)\n",
    "    df = df[df[\"image_path\"].apply(os.path.exists)].head(1000)\n",
    "    df[\"label\"] = (df[\"No Finding\"] == 1.0).astype(int)\n",
    "    \n",
    "    samples = [\n",
    "        {\"patient_id\": str(r[\"subject_id\"]), \"visit_id\": str(r[\"study_id\"]), \n",
    "         \"image\": r[\"image_path\"], \"label\": r[\"label\"]}\n",
    "        for _, r in df.iterrows()\n",
    "    ]\n",
    "    print(f\"Loaded {len(samples)} MIMIC-CXR images from {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed75fa",
   "metadata": {},
   "source": [
    "## 3. Prepare PyHealth Dataset\n",
    "\n",
    "Create a SampleDataset using `create_sample_dataset()` with ImageProcessor for the image field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f49ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_mode = \"L\" if USE_SYNTHETIC else \"RGB\"\n",
    "\n",
    "sample_dataset = create_sample_dataset(\n",
    "    samples=samples,\n",
    "    input_schema={\"image\": \"image\"},\n",
    "    output_schema={\"label\": \"binary\"},\n",
    "    input_processors={\"image\": ImageProcessor(image_size=224, mode=image_mode)},\n",
    "    dataset_name=\"chest_xray\",\n",
    ")\n",
    "\n",
    "print(f\"Total task samples: {len(sample_dataset)}\")\n",
    "print(f\"Input schema: {sample_dataset.input_schema}\")\n",
    "print(f\"Output schema: {sample_dataset.output_schema}\")\n",
    "\n",
    "train_ds, val_ds, test_ds = split_by_patient(sample_dataset, [0.7, 0.15, 0.15], seed=SEED)\n",
    "print(f\"Train/Val/Test sizes: {len(train_ds)}, {len(val_ds)}, {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591559c8",
   "metadata": {},
   "source": [
    "## 4. Inspect Batch Structure\n",
    "\n",
    "Build PyHealth dataloaders and verify the keys and tensor shapes emitted before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = get_dataloader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = get_dataloader(val_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = get_dataloader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "first_batch = next(iter(train_loader))\n",
    "\n",
    "def describe(value):\n",
    "    if hasattr(value, \"shape\"):\n",
    "        return f\"{type(value).__name__}(shape={tuple(value.shape)})\"\n",
    "    if isinstance(value, (list, tuple)):\n",
    "        return f\"{type(value).__name__}(len={len(value)})\"\n",
    "    return type(value).__name__\n",
    "\n",
    "batch_summary = {key: describe(value) for key, value in first_batch.items()}\n",
    "print(batch_summary)\n",
    "\n",
    "label_targets = first_batch[\"label\"]\n",
    "if hasattr(label_targets, \"shape\"):\n",
    "    preview = label_targets[:5].cpu().tolist()\n",
    "else:\n",
    "    preview = list(label_targets)[:5]\n",
    "print(f\"Sample labels: {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ebd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "for key, value in batch.items():\n",
    "    if hasattr(value, \"shape\"):\n",
    "        print(f\"{key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "    else:\n",
    "        print(f\"{key}: {type(value).__name__}(len={len(value)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e9f226",
   "metadata": {},
   "source": [
    "## 4.5. VisionEmbeddingModel: Output Shape Verification\n",
    "\n",
    "The core transformation performed by `VisionEmbeddingModel`:\n",
    "\n",
    "```\n",
    "Input:  (B, C, H, W)  →  e.g., (16, 1, 224, 224)\n",
    "Output: (B, P+1, E)   →  e.g., (16, 50, 128)  # P patches + 1 CLS token\n",
    "```\n",
    "\n",
    "This converts images to sequences of token embeddings, compatible with Transformer architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d35bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.models import VisionEmbeddingModel\n",
    "\n",
    "# Test all backbones and verify output shapes\n",
    "backbone_configs = [\n",
    "    (\"patch\", {\"backbone\": \"patch\", \"patch_size\": 16}),\n",
    "    (\"cnn\", {\"backbone\": \"cnn\"}),\n",
    "    (\"resnet18\", {\"backbone\": \"resnet18\", \"pretrained\": True}),\n",
    "    (\"resnet50\", {\"backbone\": \"resnet50\", \"pretrained\": True}),\n",
    "]\n",
    "\n",
    "print(\"VisionEmbeddingModel Output Shape Verification\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nInput shape: {batch['image'].shape}  # (B, C, H, W)\")\n",
    "print()\n",
    "\n",
    "for name, config in backbone_configs:\n",
    "    model = VisionEmbeddingModel(\n",
    "        dataset=sample_dataset,\n",
    "        embedding_dim=128,\n",
    "        use_cls_token=True,\n",
    "        **config,\n",
    "    )\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model({\"image\": batch[\"image\"]})\n",
    "    \n",
    "    output_shape = output[\"image\"].shape\n",
    "    info = model.get_output_info(\"image\")\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Output shape: {tuple(output_shape)}  # (B, num_tokens, E)\")\n",
    "    print(f\"  Tokens: {info['num_tokens']} = {info['num_patches']} patches + 1 CLS\")\n",
    "    print(f\"  Parameters: {n_params:,}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce223e3",
   "metadata": {},
   "source": [
    "## 5. Instantiate VisionEmbeddingModel\n",
    "\n",
    "Create the PyHealth VisionEmbeddingModel with custom hyperparameters and inspect the parameter footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344eac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.models import VisionEmbeddingModel\n",
    "\n",
    "backbone_configs = [\n",
    "    (\"patch\", {\"backbone\": \"patch\", \"patch_size\": 16}),\n",
    "    (\"cnn\", {\"backbone\": \"cnn\"}),\n",
    "    (\"resnet18\", {\"backbone\": \"resnet18\", \"pretrained\": True}),\n",
    "    (\"resnet50\", {\"backbone\": \"resnet50\", \"pretrained\": True}),\n",
    "]\n",
    "\n",
    "print(\"VisionEmbeddingModel Backbone Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, config in backbone_configs:\n",
    "    model = VisionEmbeddingModel(\n",
    "        dataset=sample_dataset,\n",
    "        embedding_dim=128,\n",
    "        use_cls_token=True,\n",
    "        **config,\n",
    "    )\n",
    "    info = model.get_output_info(\"image\")\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Tokens: {info['num_tokens']} ({info['num_patches']} patches + CLS)\")\n",
    "    print(f\"  Parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b5f6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionClassifier(nn.Module):\n",
    "    \"\"\"End-to-end classifier using VisionEmbeddingModel.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, embedding_dim=128, backbone=\"resnet18\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vision_encoder = VisionEmbeddingModel(\n",
    "            dataset=dataset,\n",
    "            embedding_dim=embedding_dim,\n",
    "            backbone=backbone,\n",
    "            pretrained=True,\n",
    "            use_cls_token=True,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embedding_dim, 1),\n",
    "        )\n",
    "        \n",
    "        self.feature_keys = [\"image\"]\n",
    "        self.label_key = \"label\"\n",
    "        self.mode = \"binary\"  \n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.vision_encoder.device\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        images = kwargs[\"image\"].to(self.device)\n",
    "        labels = kwargs[\"label\"].to(self.device).float()\n",
    "        \n",
    "        # Get embeddings: (B, num_tokens, E)\n",
    "        embeddings = self.vision_encoder({\"image\": images})\n",
    "        \n",
    "        # Use CLS token (first token) for classification\n",
    "        cls_token = embeddings[\"image\"][:, 0, :]\n",
    "        \n",
    "        # Classification head\n",
    "        logits = self.classifier(cls_token)\n",
    "        \n",
    "        # Flatten to 1D for binary classification\n",
    "        logits = logits.squeeze(-1)  # (B,)\n",
    "        labels = labels.squeeze(-1)  # (B,)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = nn.functional.binary_cross_entropy_with_logits(logits, labels)\n",
    "        \n",
    "        # Compute probabilities - must be (B,) for binary classification\n",
    "        y_prob = torch.sigmoid(logits)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"y_prob\": y_prob,  # (B,)\n",
    "            \"y_true\": labels,   # (B,)\n",
    "        }\n",
    "\n",
    "\n",
    "model = VisionClassifier(\n",
    "    dataset=sample_dataset,\n",
    "    embedding_dim=128,\n",
    "    backbone=\"resnet18\",\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Feature keys: {model.feature_keys}\")\n",
    "print(f\"Label key: {model.label_key}\")\n",
    "print(f\"Mode: {model.mode}\")\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f2d33",
   "metadata": {},
   "source": [
    "## 7. Configure Trainer\n",
    "\n",
    "Wrap the model with the PyHealth Trainer to handle optimisation, gradient clipping, and metric logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699c7e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    metrics=[\"roc_auc\"],\n",
    "    device=str(device),\n",
    "    enable_logging=False,\n",
    ")\n",
    "\n",
    "training_config = {\n",
    "    \"epochs\": 5,\n",
    "    \"optimizer_params\": {\"lr\": 1e-4},\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"monitor\": \"roc_auc\",}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad6f0f",
   "metadata": {},
   "source": [
    "## 8. Train the Model\n",
    "\n",
    "Run multiple epochs with gradient clipping, scheduler updates, and logging of loss/metrics per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    **training_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec38d7",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Validation/Test Split\n",
    "\n",
    "Switch to evaluation mode, collect predictions for validation and test splits, and compute AUROC and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = {}\n",
    "for split_name, loader in {\"validation\": val_loader, \"test\": test_loader}.items():\n",
    "    if loader is None:\n",
    "        continue\n",
    "    metrics = trainer.evaluate(loader)\n",
    "    evaluation_results[split_name] = metrics\n",
    "    formatted = \", \".join(f\"{k}={v:.4f}\" for k, v in metrics.items())\n",
    "    print(f\"{split_name.title()} metrics: {formatted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52883839",
   "metadata": {},
   "source": [
    "## 10. Inspect Sample Predictions\n",
    "\n",
    "Run a quick inference pass on the validation split to preview predicted probabilities alongside ground-truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5624ba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_loader = val_loader if val_loader is not None else train_loader\n",
    "\n",
    "y_true, y_prob, mean_loss = trainer.inference(target_loader)\n",
    "positive_prob = y_prob if y_prob.ndim == 1 else y_prob[..., -1]\n",
    "preview_pairs = list(zip(y_true[:5].tolist(), positive_prob[:5].tolist()))\n",
    "print(f\"Mean loss: {mean_loss:.4f}\")\n",
    "print(f\"Preview (label, positive_prob): {preview_pairs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f52fac",
   "metadata": {},
   "source": [
    "## 11. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741286d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SYNTHETIC:\n",
    "    shutil.rmtree(DATA_ROOT)\n",
    "    print(f\"Cleaned up: {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75c67a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### VisionEmbeddingModel\n",
    "\n",
    "Converts medical images to sequence embeddings: `(B, C, H, W) → (B, num_tokens, E)`\n",
    "\n",
    "**Backbones:**\n",
    "| Backbone | Tokens (224px) | Parameters | Use Case |\n",
    "|----------|----------------|------------|----------|\n",
    "| `patch` | 196 + CLS | ~300K | Lightweight, ViT-style |\n",
    "| `cnn` | 49 + CLS | ~100K | Good inductive bias |\n",
    "| `resnet18` | 49 + CLS | ~11M | Pretrained features |\n",
    "| `resnet50` | 49 + CLS | ~24M | Best pretrained features |\n",
    "\n",
    "**Key Features:**\n",
    "- Pretrained ImageNet weights (ResNet backbones)\n",
    "- Optional [CLS] token for classification\n",
    "- Positional embeddings\n",
    "- Compatible with PyHealth's `ImageProcessor`\n",
    "\n",
    "**Multimodal Integration:**\n",
    "```python\n",
    "# Vision: (B, P, E) where P = num_patches\n",
    "# Text: (B, T, E) where T = text_tokens  \n",
    "# EHR: (B, S, E) where S = sequence_length\n",
    "combined = torch.cat([vision_emb, text_emb, ehr_emb], dim=1)\n",
    "# → (B, P+T+S, E) → Transformer fusion\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyHealth Dev",
   "language": "python",
   "name": "pyhealth-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
